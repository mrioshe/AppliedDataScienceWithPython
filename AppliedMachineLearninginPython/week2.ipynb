{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data instances/samples/examples/feature $X$\n",
    "- Target value $y$\n",
    "- Training and test sets\n",
    "- Model/Estimator\n",
    "    - Model fitting produces a 'trained model'\n",
    "    - Training ins the process of estimating model parameters\n",
    "- Evaluation method\n",
    "- Both classification and regression take a set of training instances and learn a mapping to a target value\n",
    "- For classification, the target value is a discrete class value\n",
    "    - Binary: target value is 0 (negative class) or 1 (positive class)\n",
    "    - Multi-class: Target value is one of a set of discrete values\n",
    "    - Multi-label: There are multiple target values (labels)\n",
    "- For regression, the target value is continuous (floating point/real-valued)\n",
    "- Looking at the target value's type will guide you on what supervised learning method to use.\n",
    "- Many supervised learning methods have 'flavors' for both classification for both classification and regression\n",
    "- Simple but powerful prediction algorithms:\n",
    "    - K-nearest neighbors\n",
    "    - Linear model fit using least-squares\n",
    "- K-nearest neighbors makes few assumptions about the structure of the data and gives potentially accuarate but sometimes unstable predictions (sensitive to small changes in the training data)\n",
    "- Linear models make strong assumptions about the structure of the data and give stable but potentially inaccuarate predictions\n",
    "- Generalization ability: Refers to an algothm's ability to give accuarate predictions for new, previously unseen data.\n",
    "- Assumptions:\n",
    "    - Future unseed data (test set) will have the same properties as the current training sets.\n",
    "    - Thus, models that are accurate on the training set are expected to be accurate on the test set.\n",
    "    - But that may no happen if the trained model is tuned too specifically to the training set\n",
    "- Models that are too complex for the amount of training data avaible are said *overfit* and are not likely to generalize well to new examples.\n",
    "- Models that are too simple, that don't even do well on the training data, are said to *underfit* and also not likely to generalize well\n",
    "- Model complexity\n",
    "    - n_neighbors: number of nearest neighbors (k) to consider\n",
    "    - Default=5\n",
    "- Model fitting\n",
    "    - Metric: Distance function between data points\n",
    "        - Dafault: distance function between data points\n",
    "            - Default: Minkowski distance with power parameter p=2 (Euclidean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression: Least-Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A linear model is a sum of weighted variables that predicts a target output value given  an input data instance\n",
    "- Ex: Predicting housing prices, taxes per year ($X_{TAX}$), age in years ($X_{AGE}$)\n",
    "$$\\widehat{Y_{PRICE}}=212000+109X_{TAX}-2000X_{AGE}$$\n",
    "- A house with feature values $(X_{TAX},X_{AGE})$ of (10000,75) would have a predicted selling price of $$\\widehat{Y_{PRICE}}=212000+109(10000)-2000(75)=1152000$$\n",
    "\n",
    "- input instance - feature vector: $x=(x_0,x_1,...,x_n)$\n",
    "- predicted output:\n",
    "\n",
    "$$\\hat{y}=\\widehat{w_{0}}x_0+\\widehat{w_{1}}x_1+...\\widehat{w_{n}}x_n+\\widehat{b}$$\n",
    "\n",
    "- Parameters to estimate:\n",
    "    - $\\widehat{w}=(\\widehat{w_0},...,\\widehat{w_n}):$ feature weights/model coefficients\n",
    "    - $\\hat{b}:$ constant bias term/intercept\n",
    "\n",
    "- A linear Regression Model with one Variable (Feature)\n",
    "    - Input instance: $x=(x_0)$\n",
    "    - Predicted output: $\\hat{y}=\\hat{w_0}x_0+\\hat{b}$\n",
    "    - Parameters to stimate: \n",
    "        - $\\hat{W_0}$ (slope)\n",
    "        - $\\hat{b}$ (y intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are many different ways to estimate w and b:\n",
    "    - Different methods correspond to different \"fit\" criteria and goals and ways to control model complexity\n",
    "- The learning algorithm finds the parameters that optimize an objective function, typically to minimize some kind of loss function of the predicted target values vs target values\n",
    "- finds $w$ and $b$ that minimizes the sum of squared differentes (RSS) over the training data between predicted target and actual target values.\n",
    "- a.k.a mean squared error of the linear model\n",
    "- No parameters to control model complexity\n",
    "\n",
    "$$RSS(w,b)= \\sum_{i=1}^N{(y_i-(w-x_i+b))^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ridge regression learns $w,b$ using the same least-squares criterion but adds a penalty for large variations in w parameters\n",
    "\n",
    "$$RSS_{RIDGE}(w,b)=\\sum_{i=1}^N{(y_i-(w-x_i+b))^2}+\\alpha\\sum_{j=1}^p{w_j^2}$$\n",
    "\n",
    "- Once the parameters are learned, the ridge regression prediction formula is the same as ordinary least-squares.\n",
    "- The addition of a parameter penalty is called regularization. Regularization prevents overfitting by restricting the model, typically to reduce its complexity.\n",
    "- Ridge gression uses *L2 regularization:* minimize sum of square of w entries\n",
    "- The influence of the regularization term is controlled by the $\\alpha$ parameter.\n",
    "- Higher alpha means more regularization and simpler models.\n",
    "- Importat for some machine learning methods that all feature are on the same scale (e.g faster convergence in learning, more uniform or 'fair' influence for all weights)\n",
    "- Can also depend on the data. For now, we do MinMax scaling of the features:\n",
    "    - For each feature $x_i$: compute the min value $x_i^{MIN}$ and the max value $x_{i}^{MAX}$ achieved across all instances in the training set.\n",
    "    - For each feature: transform a given feature $x_i$ value to a scaled version $x_{i}^{'}$ using the formula\n",
    "    $$x_{i}^{'}=(x_i-x_i^{MIN})/(x_i^{MAX}-x_i^{MIN})$$\n",
    "\n",
    "Feature Normalization: The test set must use identical scaling to the training set\n",
    "\n",
    "- Fit the scaler using the training set, then apply the same scaler to transform the test set.\n",
    "- Do not scale the training and test sets using different scalers: this could lead to random skew in the data\n",
    "- Do not fit the scaler using any part of the test data: referencing the test data can lead to a form of data leakage. More on this issue later in the course.\n",
    "- Ridge regression coefficient pathway as regularization (alpha) is increased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso Regression\n",
    "\n",
    "is another form of regularized linear regression what uses an *L1 regularization* penalty for training (insted of ridge's L2 penalty)\n",
    "\n",
    "- L1 penalty: Minimize the sum of the *absolute values* of the coefficients\n",
    "\n",
    "$$RSS_{LASSO}(w,b)=\\sum_{i=1}^N{(y_i-(w-x_i+b))^2}+\\alpha\\sum_{j=1}^p{|w_j|}$$\n",
    "- This has the effect of setting parameter weights in $w$ to *zero* for least influential variables. This is called a sparse solution: a kind of feature selection\n",
    "- The parameter $\\alpha$ controls amount of L1 regularization (default=1.0)\n",
    "- The prediction formula is the same as ordinary least-squares\n",
    "- When to use ridge vs lasso regression:\n",
    "    - Many small/medium sized effects: use ridge\n",
    "    - Only a few variables with medium/large effect: use lasso\n",
    "- Lasso regression coefficient pathway as regularization (alpha) is increased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial Feature with Linear Regression\n",
    "\n",
    "$$x=(x_0,x_1) \\to x^{'}=(x_0,x_1,x_0^2,x_0x_1,x_1^2)$$\n",
    "$$\\hat{y}=\\hat{w}_0 x_0+\\hat{w}_1 x_1+\\hat{w}_{00} x_0^2+\\hat{w}_{01}x_0x_1+\\hat{w}_{11}x_1^2+b$$\n",
    "\n",
    "- Generate new feature consisting of all polynomial combinations of the original two feature $(x_0,x_1)$\n",
    "- The degree of the polynomial specifies how many variables participate at a time in each new feature (above example:degree 2)\n",
    "- This is still a weighted linear combination of features, so it's still a linear model, and can use same least-squares estimation method for $w$ and $b$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regreesion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{y}=logistic(\\hat{b}+\\hat{w_1}x_1+...\\hat{w}_nx_n)$$\n",
    "$$\\hat{y}=\\frac{1}{1+e^{-(\\hat{b}+\\hat{w}_1x_1+... \\hat{w}_nx_n)}}$$\n",
    "$$y \\in (0,1)$$\n",
    "\n",
    "- The logistic function transforms real-valued input to an output number $y$ between 0 and 1, interpreted as the probability the input object belongs to the positive class, given its input features $(x_0,x_1,...,x_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
